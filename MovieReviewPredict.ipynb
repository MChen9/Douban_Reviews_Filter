{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Distinguish 'Water Army' in Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses NLP techniques to filter reviews written by 'water army' from ordinary reviews, which needs following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba.posseg\n",
    "import jieba\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "jieba.enable_parallel(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is loading review data and user data, for the convinience of following steps, we use Jieba to \n",
    "cut sentences into words and store them in original locations(dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  status   user_name  \\\n0       tjz230     1.0          影志   \n1      7542909     1.0      翻滚吧！蛋堡   \n2  lingrui1995     1.0          凌睿   \n3      metiche     1.0  杨欢喜Metiche   \n4       kar200     1.0           觉   \n\n                                               intro  followers  follow  \\\n0  [豆瓣, 商务, 合作,  , 请, 微信, 联系,  , YOGA,  , YANG,  ...   101317.0   789.0   \n1  [撕扯, 吧,  , 趁, 青春, 未, 退役, 前,  , 谁, 在, 哭,  , 他, ...     4706.0   378.0   \n2  [我, 在, 此, 发毒, 誓,  , 我, 在, 豆瓣, 上, 从, 没收, 钱, 写, ...    23759.0    74.0   \n3  [新浪, 微博,  ,  , 杨, 欢喜, 的, 杨, 任性, 更新, 的, 公众, 号, ...     2196.0  1662.0   \n4                                        [Squeak,  ]      646.0   192.0   \n\n   watched_movies  \n0          4039.0  \n1          2346.0  \n2          2592.0  \n3          1805.0  \n4          2562.0  \n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv('/Users/mchen/Documents/18FallCourses/Independent Study/Codes/Data/ReviewsData_update.csv')\n",
    "Users = pd.read_csv('/Users/mchen/Documents/18FallCourses/Independent Study/Codes/Data/userdata_update.csv')\n",
    "ReviewsTrain = reviews.iloc[:int(reviews.shape[0]*0.8),:]\n",
    "Users = Users[Users.isna()['intro']== False]\n",
    "Users['intro'] = Users['intro'].apply(lambda x: [y for y in jieba.cut(x)])\n",
    "Users.head() # show the first 5 rows of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine whether a review is from an ordinary person, we first distinguish 'water army' from ordinary \n",
    "users. The following criterions demonstrate how to define the 'water army':\n",
    "1. From their self-intro, can we find such terms '合作', '专栏', '淘宝' etc.\n",
    "2. Do they have extraordinary number of followers and watched movies? e.g. more than 3000 followers\n",
    "\n",
    "To save the result in a dictionary, key is user name and value is 0 or 1, 0 means 'water army', 1 means 'ordinary user'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetermineUser(Users):\n",
    "    tags = ['合作', '专栏', '淘宝', '影评人', '公众', '号', '公众号','电邮','商务','微信','豆瓣','影评','短评','账号','营销','字幕']\n",
    "    UserDict = {}\n",
    "    for index, row in Users.iterrows():\n",
    "        if any(x in row['intro'] for x in tags) or row['followers']> 3000 or row['watched_movies']> 2000:\n",
    "            UserDict[row['user_name']] = 0\n",
    "        else:\n",
    "            UserDict[row['user_name']] = 1\n",
    "    return UserDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('影志', 0), ('翻滚吧！蛋堡', 0), ('凌睿', 0), ('杨欢喜Metiche', 0), ('觉', 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UserDict = DetermineUser(Users)\n",
    "list(UserDict.items())[0:5] # show the first 5 items in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews written by 'water army' are what we concern about, so labelling reviews as 'water army' is a \n",
    "pre task for training data. Since the reviews without labelled users are little helpful in this scenario, \n",
    "we can only match reviews data with labelled users and get the labelled reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetermineReview(ReviewsTrain, UserDict):\n",
    "    ReviewsLabel = {}\n",
    "    for index, row in ReviewsTrain.iterrows():\n",
    "        try:\n",
    "            ReviewsLabel[row['cid']] = UserDict[row['user_name']]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return ReviewsLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1348293405, 0),\n (1345018216, 0),\n (1348402447, 0),\n (1346991805, 0),\n (1349303778, 0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReviewsLabel = DetermineReview(ReviewsTrain, UserDict)\n",
    "list(ReviewsLabel.items())[0:5] # show the first 5 items in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following processing, we also need to cut sentences in review datasets into words, and filter some \n",
    "meaningless words by the part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ReviewsTrain = ReviewsTrain[ReviewsTrain['cid'].isin(list(ReviewsLabel.keys()))]\n",
    "pos = ['n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd', 'nrt', 'r', 'nr']\n",
    "ReviewsTrain['content'] = ReviewsTrain['content'].apply(lambda x: [y for y,f in jieba.posseg.cut(x) if f in pos])\n",
    "Articles = dict(zip(ReviewsTrain['cid'],ReviewsTrain['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After labeling reviews data, we do some calculations to translate words into numbers. In this case, we \n",
    "focus on what kinds of words occurs can support that this review is written by 'water army'. Thus, we \n",
    "simply use tf-idf to translate terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(Articles):\n",
    "    Terms = set([y for key, value in Articles.items() for y in value])\n",
    "    N = len(Articles)\n",
    "    IDF = {}\n",
    "    for term in Terms:\n",
    "        i = 0\n",
    "        for cid, article in Articles.items():\n",
    "            if term in article: i += 1\n",
    "        IDF[term] = np.log(N / (i + 1))\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('太爽', 7.0066952268370404),\n ('缓慢', 6.6012301187288767),\n ('划上', 7.0066952268370404)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = IDF(Articles)\n",
    "list(idf.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(IDF, Articles):\n",
    "    Frequency = {}\n",
    "    for cid, article in Articles.items():\n",
    "        Frequency[cid] = Counter(article)\n",
    "\n",
    "    TfIdf = {}\n",
    "    for cid, freqs in Frequency.items():\n",
    "        tfidf = {}\n",
    "        for key, freq in freqs.items():\n",
    "            tfidf[key] = IDF[key] * freq\n",
    "        TfIdf[cid] = tfidf\n",
    "    return TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1348293405,\n {'斯皮尔伯格': 3.525455137501349,\n  '他': 7.727635283981181,\n  '电影': 3.2444003280959031,\n  '梦想': 5.3019471345986151,\n  '热心': 7.0066952268370404,\n  '爱': 3.3431335807073941,\n  '情怀': 3.9621727891136174,\n  '浓缩': 6.0904044949628853,\n  '到': 2.3016797058792333,\n  '这部': 3.0651134191673504,\n  '片子': 3.5101876653705602,\n  '极具': 6.313548046277095,\n  '经典电影': 6.6012301187288767,\n  '角色': 3.2000327370667208,\n  '又': 2.5640439703467242,\n  '并茂': 7.0066952268370404,\n  '高科技': 5.7539322583416723,\n  '游戏': 5.5597229631377223,\n  '闯关': 6.0904044949628853,\n  '拿手': 6.6012301187288767,\n  '专注': 6.6012301187288767,\n  '想': 2.8246450841958342,\n  '诉说': 6.6012301187288767,\n  '都': 1.5013636909046779,\n  '这里': 4.6553199696735632,\n  '影迷': 4.2658552029118395,\n  '情': 7.0066952268370404,\n  '倾盆': 7.0066952268370404,\n  '呈现': 4.7554034282305455,\n  '谢谢': 9.8545073703144102,\n  '你': 5.0249132031124617,\n  '玩': 3.4513471653476269,\n  '我': 3.0272675669929843,\n  '推向': 5.9080829381689313,\n  '高潮': 4.2986450257348308,\n  '就': 1.9160172250672485,\n  '像': 3.2000327370667208,\n  '银幕': 4.4417458693755041,\n  '看着': 4.4809665825287848,\n  '我们': 3.3431335807073941,\n  '说': 2.6825625705820615,\n  '看': 1.791759469228055,\n  '泪': 6.313548046277095,\n  '奔': 6.313548046277095})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfIdf = TFIDF(idf, Articles)\n",
    "list(TfIdf.items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is reducing the dimension of  above embedding matrix by using SVD, which is called \n",
    "latent semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSA(TfIdf):\n",
    "    OccurenceMatrix = pd.DataFrame(TfIdf).fillna(0)\n",
    "    U, S, V = np.linalg.svd(OccurenceMatrix.values)\n",
    "    k = S.size\n",
    "    Uk = U[:, :k]\n",
    "    Sk = np.diag(S)\n",
    "    translate = np.linalg.inv(Sk) @ np.transpose(Uk)\n",
    "\n",
    "    return V.T, list(OccurenceMatrix.columns), translate, Sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step is prediction, using SVM and Decision Tree to do prediction and get accuracy of \n",
    "0.8506787330316742 and 0.5927601809954751 respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
